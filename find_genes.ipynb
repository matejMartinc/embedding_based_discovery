{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a879503-7c5e-46e6-abe2-026ca36f64f6",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5793e9ff-e2ac-44f2-844d-41bf4ec2c76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Install packages. WARNING: if you run this on windows platform install 'pytorch' \n",
    "!pip install nltk==3.7 pandas scipy==1.10.1 fasttext==0.9.2 gensim==4.3.2 scikit-learn==1.5.0 torch --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4ecd23-b63e-456b-8b46-bc3507eaf153",
   "metadata": {},
   "source": [
    "We need a Facebook MUSE library for embedding alignment. You can only get it by cloning their github repository into the project. \n",
    "Go to project folder and execute the following command in the console:\n",
    "\n",
    "    git clone https://github.com/facebookresearch/MUSE\n",
    "\n",
    "After cloning it, go to MUSE/src/utils.py file and in lines 76 and 80 change 'fastText' to 'fasttext' (i.e., change capital T to t) otherwise the code will crash. Run the Muse script for alignment of embeddings from the previous step (WARNING: replace slashes with backslashes when defining path if you have a windows platformlines):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f3c304-c01c-4fb5-bf49-f58f8bc83d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'MUSE'...\n"
     ]
    }
   ],
   "source": [
    "# Clone the Facebook MUSE repository into the current directory:\n",
    "!git clone https://github.com/facebookresearch/MUSE\n",
    "\n",
    "!sed -i 's/import fastText/import fasttext/' MUSE/src/utils.py\n",
    "!sed -i 's/return fastText\\.load_model/return fasttext.load_model/' MUSE/src/utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cb8f3d-de84-45b9-8124-5a3d74dc7e8d",
   "metadata": {},
   "source": [
    "# Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149fe840-3a49-40dc-8529-0e301b3c72d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8ffad0-2f7d-47a3-83e0-baca8afe88db",
   "metadata": {},
   "source": [
    "# Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225794d0-6d79-4f20-9b90-169109623f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create datasets for embedding training and dictionaries for embedding alignement\n",
    "def get_datasets():\n",
    "\n",
    "    pd_file = open('datasets/pd.txt', 'w', encoding=\"utf8\")\n",
    "    cr_file = open('datasets/cr.txt', 'w', encoding=\"utf8\")\n",
    "\n",
    "    with open('datasets/C_merged_PD_in_CR_trunc_clean.txt', 'r', encoding='utf8') as f:\n",
    "        vocab = {}\n",
    "        for line in f:\n",
    "            if len(line.strip()) > 0:\n",
    "                id = line.split()[0]\n",
    "                c = line.split()[1]\n",
    "                text = \" \".join(line.split()[2:]).strip()\n",
    "                text = \" \".join(nltk.wordpunct_tokenize(text)).lower()\n",
    "\n",
    "                if c == \"!CR\":\n",
    "                    cr_file.write(text + '\\n')\n",
    "                    words = text.split()\n",
    "                    for w in words:\n",
    "                        if w in vocab:\n",
    "                            vocab[w][0] += 1\n",
    "                        else:\n",
    "                            vocab[w] = [1, 0]\n",
    "                elif c == \"!PD\":\n",
    "                    pd_file.write(text + '\\n')\n",
    "                    words = text.split()\n",
    "                    for w in words:\n",
    "                        if w in vocab:\n",
    "                            vocab[w][1] += 1\n",
    "                        else:\n",
    "                            vocab[w] = [0, 1]\n",
    "        pd_file.close()\n",
    "        cr_file.close()\n",
    "    words = []\n",
    "    for word, freq in vocab.items():\n",
    "        if freq[0] > 0 and freq[1] > 2:\n",
    "            words.append((word, freq[0], freq[1]))\n",
    "\n",
    "    words = sorted(words, reverse=True, key= lambda x: x[-1])\n",
    "    train = open('datasets/en_en_dict_train.txt', 'w', encoding='utf8')\n",
    "    test = open('datasets/en_en_dict_test.txt', 'w', encoding='utf8')\n",
    "\n",
    "    counter = 0\n",
    "    for w, f1, f2, in words[:5000]:\n",
    "        if counter % 3 == 0:\n",
    "            test.write(w + '\\t' + w + '\\n')\n",
    "        else:\n",
    "            train.write(w + '\\t' + w + '\\n')\n",
    "        counter += 1\n",
    "    train.close()\n",
    "    test.close()\n",
    "\n",
    "\n",
    "get_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26889591-6ff6-47e5-a14a-994b35c222f0",
   "metadata": {},
   "source": [
    "# Embedding Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b83206-70e9-482e-9be5-6280e9732907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embeddings(input, output):\n",
    "    with open(input, \"r\", encoding=\"utf8\") as f:\n",
    "        text = \" \".join(nltk.wordpunct_tokenize(f.read())).lower()\n",
    "    filename = input.split('.')[0] + \"_preprocessed.\" +  input.split('.')[1]\n",
    "    with open(filename, \"w\", encoding=\"utf8\") as f:\n",
    "        f.write(text)\n",
    "    model = fasttext.train_unsupervised(filename, min_count=6, model='skipgram')\n",
    "    model.save_model(output + \".bin\")\n",
    "\n",
    "make_embeddings('datasets/pd.txt', 'embeddings/pd')\n",
    "make_embeddings('datasets/cr.txt', 'embeddings/cr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3141dbc1-1d0e-43c9-ab29-811e902e2b30",
   "metadata": {},
   "source": [
    "# Embedding Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fd9da1-4715-4b74-bfe9-ee498170fb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python MUSE/supervised.py --src_lang \"cr\" --tgt_lang \"pd\" --emb_dim 100 --max_vocab -1 --n_refinement 20 --dico_train \"datasets/en_en_dict_train.txt\" --dico_eval \"datasets/en_en_dict_test.txt\" --src_emb  \"embeddings/cr.bin\" --tgt_emb  \"embeddings/pd.bin\" --cuda 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46026b7f-16f3-44ab-8393-b3fae6de217d",
   "metadata": {},
   "source": [
    "The aligned embedding models will appear in the folder ./MUSE/dumped/debug/some_random_seed. Go to that folder and copy paste the files 'vectors-cr.txt' and 'vectors-pd.txt' into the embedding folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a27ef2-6d52-4e0d-bbb3-a65f49b5e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!LATEST_FOLDER=$(find MUSE/dumped/debug -type d -print0 | xargs -0 ls -td | head -n 1) && \\\n",
    "cp \"$LATEST_FOLDER/vectors-cr.txt\" \"$LATEST_FOLDER/vectors-pd.txt\" embeddings/ && \\\n",
    "echo \"Files copied to embeddings/\" || echo \"Error: Files not found in the latest folder.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1397061-d23c-4ad3-89ba-999ddc26e552",
   "metadata": {},
   "source": [
    "In the last step, we will use the aligned models to try to find novel relations between genes in the plant defense domain by using the seed relations from the circadian rhythm domain. We get 10 closest relations according to coine similarity in the plant defense domain for each seed relation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0e2ede-ddff-4f22-9b31-195fb72262a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import io\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "\n",
    "\n",
    "def get_gene_list():\n",
    "    with open(\"datasets/C_merged_PD_in_CR_trunc_clean_synonyms_B_genes_trunc.txt\", \"r\", encoding='utf8') as f:\n",
    "\n",
    "        vocab = defaultdict(int)\n",
    "        for line in f:\n",
    "            if len(line.strip()) > 0:\n",
    "                c = line.split()[1]\n",
    "                text = \" \".join(line.split()[2:]).strip()\n",
    "                text = \" \".join(nltk.wordpunct_tokenize(text)).lower()\n",
    "                if c == \"!pd\":\n",
    "                    words = text.split()\n",
    "                    for w in words:\n",
    "                        vocab[w] += 1\n",
    "\n",
    "        vocab = sorted(list(vocab.items()), reverse=True, key=lambda x: x[1])\n",
    "        vocab = [x[0] for x in vocab]\n",
    "        return set(vocab)\n",
    "\n",
    "\n",
    "def load_fasttext(emb_path, nmax=1000000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id\n",
    "\n",
    "def get_emb(word, emb, word2id):\n",
    "    avg_emb = []\n",
    "    for word_part in word.split():\n",
    "        word_emb = emb[word2id[word_part.lower()]].tolist()\n",
    "        avg_emb.append(word_emb)\n",
    "    avg = np.average(np.array(avg_emb), axis=0)\n",
    "    word_emb = avg\n",
    "    return word_emb\n",
    "\n",
    "def embeds_to_dict(emb, word2id):\n",
    "    dict = {}\n",
    "    words = list(word2id.items())\n",
    "    for w, id in words:\n",
    "        dict[w] = emb[id]\n",
    "    return dict\n",
    "\n",
    "def get_most_similar(word, word_emb, embeds, n = 10, word_list=[]):\n",
    "    neigh = []\n",
    "    items = list(embeds.items())\n",
    "    values = [v for k, v in items]\n",
    "    keys = [k for k, v in items]\n",
    "    #print(np.array(values).shape)\n",
    "    cs = cosine_similarity(word_emb.reshape(1, -1), np.array(values)).squeeze()\n",
    "    for i in range(len(keys)):\n",
    "        neigh.append((keys[i], cs[i]))\n",
    "    neigh = sorted(neigh, key=lambda x: x[1], reverse=True)\n",
    "    counter = 0\n",
    "    word_results = []\n",
    "    emb_results = []\n",
    "    for w, score in neigh:\n",
    "        if word.lower() not in w.lower() and w.lower() not in word.lower():\n",
    "            if len(word_list) == 0 or w in word_list:\n",
    "                if counter >= n:\n",
    "                    break\n",
    "                counter += 1\n",
    "                word_results.append((w, score))\n",
    "                emb_results.append(embeds[w])\n",
    "    return word_results, emb_results\n",
    "\n",
    "\n",
    "def get_all_relations(embeds, word2id):\n",
    "    gene_list = get_gene_list()\n",
    "    words = list(word2id.keys())\n",
    "    words = [x for x in words if x in gene_list]\n",
    "    diffs = {}\n",
    "    print(\"calculating all differences: \", len(words) * len(words))\n",
    "    counter = 0\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        for j in range(i + 1, len(words)):\n",
    "            word_1 = words[i]\n",
    "            word_2 = words[j]\n",
    "            emb_1 = embeds[word2id[word_1]]\n",
    "            emb_2 = embeds[word2id[word_2]]\n",
    "            diffs[word_1 + '-' + word_2] = emb_1 + emb_2\n",
    "            #diffs[word_2 + '-' + word_1] = emb_2 + emb_1\n",
    "            counter += 1\n",
    "            if counter % 1000000 == 0:\n",
    "                print(\"processing diff: \", counter)\n",
    "    print(\"Done\")\n",
    "    return diffs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_same_relations_in_domain_2(embeds_1, word2id_1, relations_1, embeds_2, word2id_2):\n",
    "\n",
    "    diffs = get_all_relations(embeds_2, word2id_2)\n",
    "\n",
    "    for rel in relations_1:\n",
    "        el1, el2 = rel\n",
    "        emb_1 = get_emb(el1, embeds_1, word2id_1)\n",
    "        emb_2 = get_emb(el2, embeds_1, word2id_1)\n",
    "        emb_rel = emb_1 + emb_2\n",
    "        word_res, emb_res = get_most_similar(el1 + '-' + el2.lower(), emb_rel, diffs, n=10, word_list=[])\n",
    "        print(\"Circadian rhythm: \", el1 + \" rel. \" + el2)\n",
    "        print(\"Most similar plant defense rel:\\n\")\n",
    "        print(\"rank\\trelation\\tcosine sim.\")\n",
    "        for idx, w in enumerate(word_res):\n",
    "            score = w[1]\n",
    "            w = w[0]\n",
    "            w = w.replace('-', ' rel. ')\n",
    "            print(str(idx + 1) + '.' + '\\t' + w + \"\\t{:.4f}\".format(score))\n",
    "        print('------------------------------------------')\n",
    "        print()\n",
    "\n",
    "\n",
    "def get_analogy(word_1, embeds_1, word2id_1, genes_1, word_2, embeds_2, word2id_2):\n",
    "\n",
    "    emb_1 = get_emb(word_1, embeds_1, word2id_1)\n",
    "    emb_2 = get_emb(word_2, embeds_2, word2id_2)\n",
    "    embeds_2 = embeds_to_dict(embeds_2, word2id_2)\n",
    "\n",
    "    for gene in genes_1:\n",
    "        emb_gene = get_emb(gene, embeds_1, word2id_1)\n",
    "        emb_result = emb_1 + emb_gene - emb_2\n",
    "        word_res, emb_res = get_most_similar(word_2.lower(), emb_result, embeds_2, n=10, word_list=get_gene_list())\n",
    "        print(\"Circadian rhythm domain: \", word_1 + ' rel. ' + gene.lower())\n",
    "        print(\"Most similar in plant defense domain:\\n\")\n",
    "        print('rank\\trelation\\tcosine sim.')\n",
    "        for idx, w in enumerate(word_res):\n",
    "            score = w[1]\n",
    "            w = w[0]\n",
    "            w = 'plant defense rel. ' + w\n",
    "            print(str(idx + 1) + '.' + '\\t' + w + \"\\t{:.4f}\".format(score))\n",
    "        print('------------------------------------------')\n",
    "        print()\n",
    "\n",
    "\n",
    "#Our seed relations\n",
    "relations_1 = [['CCA1', 'PRR7'],\n",
    "               ['CCA1', 'PRR9'],\n",
    "               ['CCA1', 'PRR5'],\n",
    "               ['CCA1', 'TOC1'],\n",
    "               ['CCA1', 'ELF3'],\n",
    "               ['CCA1', 'ELF4'],\n",
    "               ['CCA1', 'LUX'],\n",
    "               ['LHY', 'PRR7'],\n",
    "               ['LHY', 'PRR9'],\n",
    "               ['LHY', 'PRR5'],\n",
    "               ['LHY', 'TOC1'],\n",
    "               ['LHY', 'ELF3'],\n",
    "               ['LHY', 'ELF4'],\n",
    "               ['LHY', 'LUX']]\n",
    "\n",
    "\n",
    "#Path to aligned embedding models. \n",
    "path_1 = 'embeddings/vectors-cr.txt'\n",
    "path_2 = 'embeddings/vectors-pd.txt'\n",
    "nmax = 500000  # maximum number of word embeddings to load\n",
    "embeds_1, id2word_1, word2id_1 = load_fasttext(path_1, nmax)\n",
    "embeds_2, id2word_2, word2id_2 = load_fasttext(path_2, nmax)\n",
    "get_same_relations_in_domain_2(embeds_1, word2id_1, relations_1, embeds_2, word2id_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bfc3be-ebdd-4567-82f6-f01b6acc4d77",
   "metadata": {},
   "source": [
    "FOR REPRODUCIBILITY OF ORIGINAL RESULTS: In order to obtain exactly the same results as in the paper, instead of using the generated aligned embeddings, use the 'cr-aligned-original.vec' and 'pd-aligned-original.vec' files in the embedding folder. To do that, change the paths to the embedding files in the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca25364-016d-441c-a7cc-a098b078b1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to exactly reproduce the results in the paper, instead use the 'cr-aligned-original.vec' and 'pd-aligned-original.vec' files in the embedding folder.\n",
    "path_1 = 'embeddings/cr-aligned-original.vec'\n",
    "path_2 = 'embeddings/pd-aligned-original.vec'\n",
    "nmax = 500000  # maximum number of word embeddings to load\n",
    "embeds_1, id2word_1, word2id_1 = load_fasttext(path_1, nmax)\n",
    "embeds_2, id2word_2, word2id_2 = load_fasttext(path_2, nmax)\n",
    "get_same_relations_in_domain_2(embeds_1, word2id_1, relations_1, embeds_2, word2id_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
